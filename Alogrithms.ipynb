{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from numpy import linalg as LA\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, alpha = 0.01, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 5000):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "        \talpha is the learning rate\n",
    "        \tregLambda is the regularization parameter\n",
    "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "        \tepsilon is the convergence parameter\n",
    "        \tmaxNumIters is the maximum number of iterations to run\n",
    "        '''\n",
    "        self.alpha= alpha\n",
    "        self.regLambda = regLambda\n",
    "        self.regNorm = regNorm \n",
    "        self.epsilon = epsilon \n",
    "        self.maxNumIters = maxNumIters\n",
    "        self.theta = None\n",
    "        self.theta_final=   None\n",
    "        self.hist= None \n",
    "        \n",
    "\n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "        n,d = X.shape\n",
    "        if self.regNorm == 2 :\n",
    "            cost = (-y.T * np.log(self.sigmoid(X*theta)) - (1.0-y).T* np.log( 1.0 - self.sigmoid(X*theta))) + (self.regLambda*(theta.T * theta)/ 2.0)  \n",
    "        \n",
    "        elif self.regNorm == 1 :\n",
    "            cost = (-y.T * np.log(self.sigmoid(X*theta)) - (1.0-y).T* np.log( 1.0 - self.sigmoid(X*theta))) + (self.regLambda*(np.sum(theta))/ 2.0) \n",
    "            \n",
    "        c= cost.item((0,0))\n",
    "#        print(c)\n",
    "#        \n",
    "        return c\n",
    "\n",
    "    \n",
    "    \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "        X = np.array(X)\n",
    "        n,d = X.shape\n",
    "        yhat = self.sigmoid(X* theta)\n",
    " \n",
    "        if self.regNorm == 2 :\n",
    "             gradient = (X.T * (yhat - y) ) + (regLambda*theta)\n",
    "        else :\n",
    "             gradient = (X.T* (yhat - y) ) + (regLambda *  (np.sign(np.array(theta))))  \n",
    "            \n",
    "        \n",
    "        gradient[0] = sum(self.sigmoid(X*theta)- y )\n",
    "        \n",
    "#        print(gradient.shape)\n",
    "        return gradient \n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "        '''\n",
    "        \n",
    "        n,d = X.shape\n",
    "        X = np.c_[np.ones((n,1)), X]\n",
    "\n",
    "        self.theta=  np.matrix(np.random.randn((d+1))).T\n",
    "        print(self.theta.shape)\n",
    "        theta_updated = self.theta\n",
    "        theta_old = self.theta\n",
    "        j=0\n",
    "        jp=[]\n",
    "        self.hist=[]\n",
    "        while j < self.maxNumIters:\n",
    "            \n",
    "            theta_updated = theta_old - (self.alpha* (self.computeGradient(self.theta, X, y, self.regLambda)))\n",
    "           \n",
    "            if (self.hasConverged(theta_old,theta_updated)):\n",
    "                self.theta = theta_updated \n",
    "                \n",
    "                break\n",
    "            \n",
    "            else :\n",
    "                theta_old = theta_updated\n",
    "                j=j+1\n",
    "#                print(j)\n",
    "                jp.append(j)\n",
    "                cost = self.computeCost(theta_updated, X, y, self.regLambda)\n",
    "                self.hist.append(cost)\n",
    "#                print(cost)\n",
    "                self.theta = theta_updated\n",
    "                \n",
    "       \n",
    "    \n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "        Returns:\n",
    "            an n-dimensional numpy vector of the predictions\n",
    "        '''\n",
    "        n,d= X.shape\n",
    "#        print(self.theta)\n",
    "        X = np.c_[np.ones((n,1)), X]\n",
    "\n",
    "        y_pred=self.sigmoid(np.matmul(X,self.theta))\n",
    "\n",
    "        \n",
    "# making binary predictions\n",
    "        for i in range(y_pred.shape[0]):\n",
    "            if y_pred[i] > 0.5:\n",
    "                y_pred[i] = 1\n",
    "            else :\n",
    "                y_pred[i]=0 \n",
    "                \n",
    "        return np.array(y_pred)\n",
    "\n",
    "# convrting it to sigmmoid function \n",
    "    def sigmoid(self, Z):\n",
    "        \n",
    "        return 1.0/(1.0+np.exp(-Z))\n",
    "    \n",
    "# checks for convergence        \n",
    "    def hasConverged(self,theta_old,theta_updated):\n",
    "        '''\n",
    "        checks for a n epsilon value\n",
    "        '''\n",
    "        n = theta_updated - theta_old\n",
    "        if LA.norm(n) <= self.epsilon:\n",
    "            return 1 \n",
    "        else :\n",
    "            return 0\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    TEMPLATE FOR MACHINE LEARNING HOMEWORK\n",
    "    AUTHOR Eric Eaton\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "class LogisticRegressionAdagrad:\n",
    "\n",
    "    def __init__(self, alpha = 0.0001, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 10):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "            alpha is the learning rate\n",
    "            regLambda is the regularization parameter\n",
    "            regNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "            epsilon is the convergence parameter\n",
    "            maxNumIters is the maximum number of iterations to run\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.regLambda = regLambda\n",
    "        self.regNorm = regNorm\n",
    "        self.epsilon = epsilon\n",
    "        self.maxNumIters =  maxNumIters\n",
    "        self.JHist = None\n",
    "        self.theta = None\n",
    "        self.G = None\n",
    "        \n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "        yhat = self.sigmoid(X*theta)\n",
    "        if (self.regNorm == 2):\n",
    "            norm_term = (self.regLambda/2.0)* (theta.T*theta )\n",
    "        else:\n",
    "            norm_term = (self.regLambda/2.0)* (np.sum(theta))\n",
    "        J = -(y.T*np.log(yhat)) - ((1.0-y).T*(np.log(1.0 - yhat))) + norm_term  \n",
    "        J_scalar = J.item((0,0))\n",
    "        return J_scalar\n",
    "       \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "        yhat = self.sigmoid(X*theta)\n",
    "        gradient = (X.T* (yhat - y))\n",
    "        # Account for regularization in theta\n",
    "        if (self.regNorm == 2):\n",
    "            for k in range(1,len(theta)):\n",
    "                gradient[k]= gradient[k] + (regLambda*theta[k])\n",
    "        else:\n",
    "            for k in range(1,len(theta)):\n",
    "                gradient[k]= gradient[k] + (regLambda*np.sign(theta[k]))\n",
    "        gradient[0] = sum(yhat-y)\n",
    "        return gradient\n",
    "            \n",
    "    def fit(self,X_train,X_test, y,y_test):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "        '''\n",
    "        n,d = X_train.shape\n",
    "        # Add bias to the feature space\n",
    "        X_std = np.c_[np.ones((n,1)), X_train]\n",
    "        # Intialize variables for regression\n",
    "        self.theta = np.matrix(np.random.randn((d + 1))).T\n",
    "        X_copy = X_std\n",
    "        X_copy = np.c_[X_std,y]\n",
    "        self.G = np.zeros((d+1,1))\n",
    "        lc=[]\n",
    "        for i in range(self.maxNumIters):\n",
    "            np.random.shuffle(X_copy)\n",
    "            X_sample = X_copy[:,0:d+1]\n",
    "            y_sample = X_copy[:,-1]\n",
    "            for k in range(n):   \n",
    "                theta_old = self.theta\n",
    "                X_new = X_sample[k].reshape(1,d+1)\n",
    "                gradient = self.computeGradient(self.theta,X_new,y_sample[k], self.regLambda)\n",
    "                # Adapt the alpha by the past values of gradient\n",
    "                self.G = self.G + np.square(gradient)\n",
    "                for k in range(0,len(gradient)):\n",
    "                     self.theta = self.theta - (self.alpha * gradient/(np.sqrt(self.G[k]) + 1e-4))\n",
    "                        \n",
    "            if(i % 100 == 0):\n",
    "                y_pred = self.predict(X_test)\n",
    "                acc = accuracy_score(y_pred,y_test)\n",
    "                lc.append((i,acc))\n",
    "        return lc\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "        Returns:\n",
    "            an n-dimensional numpy vector of the predictions\n",
    "        '''\n",
    "        \n",
    "        # Add bias to the feature space\n",
    "        X_std = np.c_[np.ones((X.shape[0],1)),X]\n",
    "        # Predict the labels\n",
    "        yhat = self.sigmoid(X_std.dot(self.theta))\n",
    "        for i in range(yhat.shape[0]):\n",
    "            if (yhat[i] > 0.5):\n",
    "                yhat[i] = 1\n",
    "            else:\n",
    "                yhat[i] = 0\n",
    "        yhat = np.asarray(yhat)\n",
    "        yhat = yhat.reshape((yhat.shape[0],1))\n",
    "        return yhat\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        '''\n",
    "        Computes the sigmoid function 1/(1+exp(-z))\n",
    "        '''\n",
    "        return 1.0/(1.0 + np.exp(-Z))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "\"WDBC Data\"\n",
    "wb_data= pd.read_csv(\"data/wdbc.dat\",sep = \",\",header = None)\n",
    "\n",
    "######################################################################################################\n",
    "df1, df2, df3,df4,df5 = np.split(wb_data.iloc[:-4,:],5)\n",
    "train_1 = pd.concat([df1,df2,df3,df4])\n",
    "test_1 = df5\n",
    "train_2 = pd.concat([df1,df2,df3,df5])\n",
    "test_2 = df4\n",
    "train_3 = pd.concat([df1,df2,df5,df4])\n",
    "test_3 = df3\n",
    "train_4 = pd.concat([df1,df5,df3,df4])\n",
    "test_4 = df2\n",
    "train_5 = pd.concat([df5,df2,df3,df4])\n",
    "test_5 = df1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X = wb_data.iloc[:,:-1]\n",
    "# X = np.array(X, dtype=\"float64\")\n",
    "# y = np.array(wb_data[30],dtype=object)\n",
    "# y = y.astype(str)\n",
    "# for j in range(y.shape[0]):\n",
    "#     if y[j] == 'M':\n",
    "#         y[j] = 1\n",
    "#     else:\n",
    "#         y[j] = 0\n",
    "# y = y.astype(int).reshape(y.shape[0],1)\n",
    "\n",
    "clf1= LogisticRegressionAdagrad(maxNumIters = 1500, regLambda = 0.001)\n",
    "clf2= LogisticRegressionAdagrad(maxNumIters = 1500, regLambda = 0.01)\n",
    "# r1 = clf1.fit(X,y)\n",
    "# x1= []\n",
    "# y1=[]\n",
    "# for i in range(len(r1)):\n",
    "#     x1.append(r1[i][0])\n",
    "#     y1.append(r1[i][1])\n",
    "# plt.plot(x1,y1)\n",
    "\n",
    "# y = y.astype(int).reshape(y.shape[0],1)\n",
    "# r2 = clf2.fit(X,y)\n",
    "# x2= []\n",
    "# y2=[]\n",
    "# for i in range(len(r2)):\n",
    "#     x2.append(r2[i][0])\n",
    "#     y2.append(r2[i][1])\n",
    "# plt.plot(x2,y2)\n",
    "# plt.show()\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "# accTrain = []\n",
    "# #     for i in range(5):\n",
    "train = [train_1,train_2,train_3,train_4,train_5]\n",
    "test = [test_1,test_2,test_3,test_4,test_5]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = train[i].iloc[:,:-1]\n",
    "    X_train = np.array(X_train, dtype=\"float64\")\n",
    "    y_train = np.array(train[i][30],dtype=object)\n",
    "    y_train = y_train.astype(str)\n",
    "    for j in range(y_train.shape[0]):\n",
    "        if y_train[j] == 'M':\n",
    "            y_train[j] = 1\n",
    "        else:\n",
    "            y_train[j] = 0\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    X_test = test[i].iloc[:,:-1]\n",
    "    X_test = np.array(X_test, dtype=\"float64\")\n",
    "    y_test = np.array(test[i][30],dtype=object)\n",
    "    y_test = y_test.astype(str)\n",
    "    for j in range(y_test.shape[0]):\n",
    "        if y_test[j] == 'M':\n",
    "            y_test[j] = 1\n",
    "        else:\n",
    "            y_test[j] = 0\n",
    "    y_test = y_test.astype(int)\n",
    "    # Standardize the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train- mean) / std\n",
    "    X_test = (X_test - mean)/std\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    acc1 = clf1.fit(X_train, X_test,y_train,y_test)\n",
    "    acc2 = clf2.fit(X_train, X_test,y_train,y_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the learnign curves\n",
    "\n",
    "x1 = []\n",
    "y1 = []\n",
    "x2 = []\n",
    "y2 = []\n",
    "\n",
    "for i in range(len(acc1)):\n",
    "    x1.append(acc1[i][0])\n",
    "    y1.append(acc1[i][1])\n",
    "plt.plot(x1,y1,'orange')\n",
    "for i in range(len(acc2)):\n",
    "    x2.append(acc2[i][0])\n",
    "    y2.append(acc2[i][1])\n",
    "plt.plot(x2,y2,'green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sailalitha\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1)\n",
      "(20, 1)\n",
      "(20, 1)\n",
      "(20, 1)\n",
      "0.5956521739130435\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "\"Retinopathy Data\"\n",
    "r_data = pd.read_csv(\"data/retinopathy.dat\",sep = \",\",header = None)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "df1,df2,df3,df4,df5 = np.split(r_data.iloc[:-1,:],5)\n",
    "train_1 = pd.concat([df1,df2,df3,df4])\n",
    "test_1 = df5\n",
    "train_2 = pd.concat([df1,df2,df3,df5])\n",
    "test_2 = df4\n",
    "train_3 = pd.concat([df1,df2,df5,df4])\n",
    "test_3 = df3\n",
    "train_4 = pd.concat([df1,df5,df3,df4])\n",
    "test_4 = df2\n",
    "train_5 = pd.concat([df5,df2,df3,df4])\n",
    "test_5 = df1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "C = 0.01  # value of C for the SVMs\n",
    "# create an instance of SVM with the custom kernel and train it\n",
    "clf = LogisticRegression(regNorm=2)\n",
    "\n",
    "acc = []\n",
    "# #     for i in range(5):\n",
    "train = [train_1,train_2,train_3,train_4,train_5]\n",
    "test = [test_1,test_2,test_3,test_4,test_5]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = train[i].iloc[:,:-1]\n",
    "    X_train = np.array(X_train, dtype=\"float64\")\n",
    "    y_train = np.array(train[i][19],dtype=object)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = test[i].iloc[:,:-1]\n",
    "    X_test = np.array(X_test, dtype=\"float64\")\n",
    "    y_test = np.array(test[i][19],dtype=object)\n",
    "    y_test = y_test.astype(int)\n",
    "    # Standardize the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train- mean) / std\n",
    "    X_test = (X_test - mean)/std\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    acc.append(accuracy)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1)\n",
      "(9, 1)\n",
      "(9, 1)\n",
      "(9, 1)\n",
      "(9, 1)\n",
      "0.7725490196078432\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "\"Diabetes Data\"\n",
    "d_data = pd.read_csv(\"data/diabetes.dat\",sep = \",\",header = None)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "df1,df2,df3,df4,df5 = np.split(d_data.iloc[:-3,:],5)\n",
    "train_1 = pd.concat([df1,df2,df3,df4])\n",
    "test_1 = df5\n",
    "train_2 = pd.concat([df1,df2,df3,df5])\n",
    "test_2 = df4\n",
    "train_3 = pd.concat([df1,df2,df5,df4])\n",
    "test_3 = df3\n",
    "train_4 = pd.concat([df1,df5,df3,df4])\n",
    "test_4 = df2\n",
    "train_5 = pd.concat([df5,df2,df3,df4])\n",
    "test_5 = df1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(regNorm=1)\n",
    "acc = []\n",
    "# #     for i in range(5):\n",
    "train = [train_1,train_2,train_3,train_4,train_5]\n",
    "test = [test_1,test_2,test_3,test_4,test_5]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = train[i].iloc[:,:-1]\n",
    "    X_train = np.array(X_train, dtype=\"float64\")\n",
    "    y_train = np.array(train[i][8],dtype=object)\n",
    "    y_train = y_train.astype(str)\n",
    "    for j in range(y_train.shape[0]):\n",
    "        if y_train[j] == 'tested_positive':\n",
    "            y_train[j] = 1\n",
    "        else:\n",
    "            y_train[j] = 0\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    X_test = test[i].iloc[:,:-1]\n",
    "    X_test = np.array(X_test, dtype=\"float64\")\n",
    "    y_test = np.array(test[i][8],dtype=object)\n",
    "    y_test  = y_test.astype(str)\n",
    "    for j in range(y_test.shape[0]):\n",
    "        if y_test[j] == 'tested_positive':\n",
    "            y_test[j] = 1\n",
    "        else:\n",
    "            y_test[j] = 0\n",
    "    y_test = y_test.astype(int)\n",
    "    # Standardize the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train- mean) / std\n",
    "    X_test = (X_test - mean)/std\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    acc.append(accuracy)\n",
    "print(np.mean(acc))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"Learning Curve over different Models on retinopathy data\"\n",
    "###########################################################################################\n",
    "\n",
    "n = [100,1000,2000,5000,10000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
